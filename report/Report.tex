\documentclass[11pt]{article} % do not change this line
\input{BigDataStyle.txt}      % do not change this line
\usepackage[hidelinks]{hyperref}
\usepackage[export]{adjustbox}
\usepackage{amsmath,amsfonts,amssymb,amsthm,latexsym,graphicx,url,bookmark,enumitem,subcaption,color,xcolor,listings,hhline,array}

\emergencystretch=5mm
\tolerance=400
\allowdisplaybreaks[4]

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{problem}[theorem]{Problem}

\theoremstyle{definition}
\newtheorem*{remark}{Remark}

\title{Hedging Optimisation with K-means Clustering on Cryptocurrencies}
\author{Chengkai Lu}

\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

\newcommand{\Programme}{Data Science and Analytics with a Year in Industry}

\newcommand*{\figuretitle}[1]{
  {
    \centering
    \text{#1}
    \par\medskip
  }
}

\lstset{basicstyle=\ttfamily,
    showstringspaces = false,
    commentstyle = \color{red},
    keywordstyle = \color{blue},
    otherkeywords = {docker-compose, mvn, jupyter, pdflatex}
}

\pagenumbering{roman}

\begin{document}
\maketitle

\declaration

\begin{abstract}
\thispagestyle{plain}
Cryptocurrencies like Bitcoin and Ripple are becoming popular in these years. They can be obtained through mining or transactions. Every individual cryptocurrency can be transferred directly using a public key in digital wallets. Their price is mostly tied to supply/demand and hard to be interfered by governments. In addition, cryptocurrencies have some dependencies because some of them like Dogecoin can only be bought by some major cryptocurrencies such as Bitcoin and Ethereum in cryptocurrency exchanges.

Value at Risk is a popular traditional method in financial technical analysis for estimating potential total risks of a financial portfolio. A well-estimated risk can prevent an investor or a financial institution from losing more than their expectation. In reality, the financial markets sometimes are unpredictable. Investors are striving for putting all the risks under their control.

K-means clustering is a popular unsupervised learning algorithm for grouping unlabelled data. It aims on finding the natural way of separating observations into diffident clusters and is quite popular in the area of marketing for discovering potential customer behaviour. However, it might be also applicable to group stocks/cryptocurrencies having similar price movement together. It states that a systematic way of hedging is achievable.

A modern machine learning algorithm for clustering investments before a traditional technical analysis might improve the performance of risk estimation. A better hedging can prevent investors from suffering disastrous loss. Especially in the market of cryptocurrency, the price movement is more irrational and abnormal because of lacking regulation. This project will try to research if the optimisation of risk estimation performs well in the market of cryptocurrency.
\end{abstract}

\listoffigures
\listoftables
\setcounter{page}{4}

\clearpage

\section*{\hfil Acknowledgement \hfil}

Thanks to Dr. Yuri Kalnishkan for his supervision and guidance on this project. He shows his passion and expertise in the area of machine learning, data analysis and finance. His effort and comprehensive knowledge have guided this project and myself onto the right path of concrete analysis on financial data. In addition, thanks to Professor Alberto Paccanaro for his help in the early stages of this project and introducing me to Dr. Yuri Kalnishkan.

Some of the contents referenced the slides and notes of CS5100-Data Analysis and CS5920-Machine Learning which are compiled by Professor Vladimir Vovk and Professor Zhiyuan Luo. Any other references have been clarified in the end of this project on the Reference section. This project is used for non-profit purposes. Any other uses of this project should be informed, and the author reserves the right to refuse any inappropriate or illegal usage.

\clearpage

\pagenumbering{arabic}

\section{Introduction}
\setcounter{page}{1}

Hedging is an important investing strategy to avoid risks that will result in substantial losses or gains. Investors have been developing and implementing traditional technical analysis on evaluating risks of an investment, but it is still tricky to capture some events such as a financial crash which breaks the previous risk estimation. Some of the tasks are computationally expensive and human beings are hard to achieve, therefore, computer science and machine learning has been used for improving the performance of analysis.

Machine learning has become a popular area which focuses on enabling machines or computers to learn and discover the patterns. It can be applied to different domains as long as there exists data. Especially in the financial area, people are striving for using modern computing power to deal with problems efficiently because time is a crucial factor in finance, and this is called computational finance.

\subsection{Motivation}

When we talk about the risk of an investment, we are normally curious about the risk for the future. Prediction of risks can be tricky because a randomly unpredictable event will impact on the market movement. An unexpected financial event might collapse the market and result in a crash. All the products in the market will be influenced by the risks. These kinds of risks are called \textsl{Systematic Risk}, which cannot be avoided through a clustered investment portfolio. However, we can spread the risks over diffirent investments by generating a well-clustered investment portfolio.

Traditional risk management methods have been developed for centuries and they can produce reasonable result of hedges, but there still exists possibilities to improve the performance. Machine learning are also used for avoidance of risk or market prediction. We want to seek for a way of improving traditional risk management methods by applying machine learning algorithms on top.

\subsection{Aims And Objectives}

Value at Risk is a traditional method which can evaluate the risk of a investment portfolio or a single investment. It is widely used by a large number of financial institutions, especially banks because it provides a quantitative criterion for risk estimation. However, the risk might be overestimated or underestimated since the products in a portfolio might have varied volatilities and activities. There also might be some events that have happened on one product, and another product might experience it in the future. For example, we might apply VaR of the combination of two products with similar activities and cover the worst/best cases of a product. Another product will then have the ability to consider the possible worst cases.

A clustering of investment before generating a portfolio can remove the concern of wrongly evaluating the risks. Well-clustered financial portfolios will ensure that products in a investment combination have similar risks or volatilities.

K-means clustering is an appropriate method for putting financial investments into different groups. It can create natural grouping on the financial products by putting those with similar price volatilities together. That is, it looks at the shape of each product along the timeline and estimates the similarity between two lines to decide if the two products should be in the same cluster.

This project aims on seeing if K-means clustering can group financial products with their daily volatilities and improve the result of risk estimation on cryptocurrencies. In addition, it will examine if the effect on cryptocurrencies is different from that on stocks.

\subsection{Project Structure}

The project includes several parts, experiment, implementation of the analysis and report. The experiment is for developing and adjusting the analysis. After receiving a satisfactory result from the experiment, the analysis will be implemented on a minimal production-level basis. This report will then present the most critical detail of the analysis.

\subsubsection{Technologies}

In order to facilitate the process of analysis, we use several technologies to develop this project. Scalability is also important for a proper project. As a result, we will build a well-structured programme that meet the minimal requirement of scalability and can be productionised in the future.

In this project, the following technologies are used for different purposes:
\begin{itemize}
  \item Programming Languages
  \begin{itemize}
    \item \textsl{Python} : A popular programming language for statistics, machine learning and data analysis due to its flexibility, powerful packages, ease of learning and data visualisation. We use Python for most of the analysis because it is handy for quick experiments.
    \item \textsl{Java} : Many companies are using Java as the main languages for their servers and applications because of its efficiency, performance and stability. We use Java for the ETL pipeline and a preparation of productionising the result of analysis.
  \end{itemize}

  \item Database
  \begin{itemize}
    \item \textsl{PostgreSQL}\cite{postgresql} : We use PostgreSQL as our database because it is well-developed and supports efficient quaries.
  \end{itemize}

  \item Containerisation
  \begin{itemize}
    \item \textsl{Docker}\cite{docker} : An open source operating-system-level virtualisation tool to build an environment for running programmes. It not only provides an diffirent environment such as Linux on a local machine, but makes the same configuration and environment portable. We use Docker for a quick construction of PostgreSQL database.
  \end{itemize}

  \item Version Control
  \begin{itemize}
    \item \textsl{Git}\cite{git} : A popular tool for version control. The project uses it for managing records of development.
    \item \textsl{GitHub}\cite{github} : All the records and resources of this project are stored into a private repository on GitHub which is a website for storing local Gits onto the cloud.
  \end{itemize}

  \item Frameworks
  \begin{itemize}
    \item \textsl{Apache Spark}\cite{spark} : An open source framework for cluster computing. This is for preparation of productionisation of this project and changing the scripting python code to an automated programme. We will use Spark SQL to accelerate reading data from the database and the MLlib for analysis with machine learning.
    \item \textsl{Spring Framework}\cite{spring} : An open source Java framework for implementation of inversion of control. We use it to facilitate programming process and for preparation of building a web server in the future.
  \end{itemize}

  \item Project Management
  \begin{itemize}
    \item \textsl{Apache Maven}\cite{maven} : A tool for project management in Java. It can create an automatic pipeline for installing libraries, packaging programmes, managing package dependencies, etc.
  \end{itemize}

  \item Python Packages
  \begin{itemize}
    \item \textsl{Numpy}\cite{scipy} : Array manipulation.
    \item \textsl{Pandas}\cite{scipy} : Data manipulation and analysis.
    \item \textsl{Scikit-Learn}\cite{scipy} : Machine learning and data mining.
    \item \textsl{Tslearn}\cite{tslearn} : Time series data processing and analysis.
    \item \textsl{Matplotlib}\cite{scipy} : Data visualisation.
  \end{itemize}

  \item Java Libraries
  \begin{itemize}
    \item \textsl{Joda-Time}\cite{jodatime} : Datetime and timestamps manipulation.
    \item \textsl{Project Lombok}\cite{lombok} : Being used for simplifying code and logging.
    \item \textsl{Gson}\cite{gson} : Json format data processing.
    \item \textsl{Apache HttpComponents}\cite{httpcomponents} : Asynchronous http request.
  \end{itemize}

\end{itemize}

\subsubsection{Programme Structure}

The programme is mainly composed by the following 3 parts:
\begin{itemize}
    \item Database
    \item Java Server
    \item Jupyter Notebook
\end{itemize}

The database is built by Docker and PostgreSQL. The \textsl{\textbf{Dockerfile}} under the \textsl{\textbf{$<$root$>$/db/}} directory contains the script for building a PostgreSQL image and implementing the \textsl{\textbf{$<$root$>$/db/init.sql}} SQL query for database initialisation. Then we use docker-compose to generate the container of the image and assign the port to the local container. The \textsl{\textbf{$<$root$>$/docker-compose.yml}} file owns the configuration. Also, there is a \textsl{\textbf{$<$root$>$/.env-dev}} file containing the environment variables of the Docker container. The database will set up the database name, database user and the database password through referencing these variable settings. This local database is used for temporary development purposes. There are two major schemas which are input and output. All the input and transformed data will be stored into the input schema. The output schema is used to store the output data once we have productionised the analysis. The website or the prgramme being built in the future can then access the data and visualise them onto the website.

We use Maven for managing our Java project. It facilitates the process of acquiring libraries and managing the dependencies between projects or services. The \textsl{\textbf{$<$root$>$/parent/pom.xml}} manages the Java programme. It controls the versions of dependencies and the project hierarchy. The main services are in \textsl{\textbf{$<$root$>$/analysis/}}. The \textsl{\textbf{$<$root$>$/analysis/pom.xml}} contains the dependencies of the analysis server. We use Spring Framework for development and database access in the future. The \textsl{\textbf{application.properties}} in \textsl{\textbf{$<$root$>$/analysis/src/main/resources/}} holds the database and Spark configuration. Spring Boot is used for preparation of productionisation with websites. It can help us quickly build up a single page web application in the future.

There are mainly 2 services currently, which are ETL and analysis. \textsl{\textbf{Application.java}} is the entry point. If we keep running the server, it will run the data updates and analysis routinely every day. \textsl{\textbf{TaskRunner.java}} is responsible for arrangement of all the tasks. The tasks and classes for ETL are in the \textsl{\textbf{com.kenlu.crypto.extraction}} package. \textsl{\textbf{com.kenlu.crypto.domain}} contains the data models and enums. Analysis are implemented in the \textsl{\textbf{com.kenlu.crypto.analysis}} package. However, currently, we only implement the ETL service because the productionisation is not completed yet.

All the experiments and visualisations are writen in Python and stored under \textsl{\textbf{$<$root$>$/experiment/}}. The 2 main scripts are \textsl{\textbf{var\_crypto.ipynb}} and \textsl{\textbf{var\_stock.ipynb}} with the format of Jupyter Notebook. The scripts import the data either from the local database or the csv files in case the database connection or the API of cryptos and stocks collapse. The result of these experiments will be transformed to a productionised version in Java once it is ready.

\subsubsection{Report Structure}

This report is mainly composed by several parts including an introduction, a background research, analysis and a conclusion. We use LaTeX for facilitation of the writing process. \textsl{\textbf{$<$root$>$/report/}} contains all the LaTeX files and resources for compilation of the report. \textsl{\textbf{Report.tex}} includes the main scripts. \textsl{\textbf{$<$root$>$/report/resources/}} holds the images for the report. After compilation, a \textsl{\textbf{Report.pdf}} file will be built for a complete presentation.

\section{Background Research}
\subsection{Value at Risk}
Value at Risk (VaR) is a method used to evaluate the risk of a financial portfolio. It summarises the degree of investment risk with a single number. Financial institutions have been widely using VaR as a metric to decide how much capital they should keep to bear with risks.

Financial specialists are usually interested in the statement which indicates a specific criterion that helps there decision making. As a result, VaR is normally interpreted as follow:

\textsl{"I have X\% of confidence that the loss of our investment will not be more than \$V in the next N days."}

{
  \footnotesize
  where:
  \begin{itemize}[label=-, leftmargin=4em, itemsep=0.1em]
    \item ${X}$ is the confidence level
    \item ${V}$ is the VaR of the portfolio
    \item ${N}$ is the time horizon
  \end{itemize}
}

A proper VaR states that there will only be a (100 - X)\% of chance that our loss of investment will be exceeded. For example, given V is \$100,000, X is 95, and N is 10, there will only be 5\% of scenarios that the loss of our portfolio will be more than \$100,000 in the next 10 days based on our estimation.

An N-day VaR\eqref{eq:2.1} is usually calculated on the basis of 1-day VaR:

\begin{equation}
  \label{eq:2.1}
  \tag{2.1}
  {\text{N-day VaR} = \text{1-day VaR} \times \sqrt{N}}
\end{equation}

There are several approaches for calculating VaRs. In this project, as our objective is to see if clustering algorithms can improve the VaR estimation, we will only talk about the simplest one - \textsl{historical simulation approach}\cite{john/ofaod:2017}.

\subsubsection{Historical Simulation Approach}

Historical simulation approach looks at historical data and simulate the past  scenarios that might happen in the future. Then it chooses the worst (100 - X)\% of scenarios as the worst cases of the loss of our investment where the $(100 - X)^{\mathrm{th}}$ percentile is our VaR.

For example, we want to estimate the 1-day VaR with 95\% of confidence for tomorrow, and there are 500 days of historical data. The data for each day is the return/loss from the previous day, which means there are 500 scenarios\eqref{eq:2.2} that might happen tomorrow. We can then multiply the value of investment we are holding today by the $5^{\mathrm{th}}$ least return rate (most loss rate) and gain the loss that might happen tomorrow as our 1-day VaR\cite{john/ofaod:2017}.

{
  \begin{equation}
    \label{eq:2.2}
    \tag{2.2}
    {\text{Value under } i^{\mathrm{th}} \text{ scenario } = v_{n}\frac{v_{i}}{v_{i-1}}}
  \end{equation}

  \footnotesize
  where:
  \begin{itemize}[label=-, leftmargin=4em, itemsep=0.1em]
    \item $v_{n}$ is the value of today
    \item $v_{i}$ is the value of Day $i$
  \end{itemize}
}

\subsection{Correlation Analysis}

In statistics, covariance\eqref{eq:2.3} defines the variability between two individual attributes, which means the level of influence from one feature to another. The numbers correspond to similarity/dissimilarity of the two variables. Positive numbers represent a similar behaviour between them, and vice versa\cite{wiki/cov:2018}.

{
  \begin{equation}
    \label{eq:2.3}
    \tag{2.3}
    {\operatorname {cov} (X,Y)=\operatorname {E} [(X-\mu _{X})(Y-\mu _{Y})]}
  \end{equation}

  \footnotesize
  where:
  \begin{itemize}[label=-, leftmargin=4em, itemsep=0.1em]
    \item ${E}$ is the expectation
    \item ${X, Y}$ are vectors of all the samples
    \item $\operatorname {cov}$ is the covariance
    \item ${\mu}_{X}$ is the mean of X
    \item ${\mu}_{Y}$ is the mean of Y
  \end{itemize}
}

However, if we want to measure the strength of the linear relationship in between, covariance is not enough. We also need to consider the variance in each feature to tell whether the linear relationship is strong. In this case, the correlation was introduced. Correlation is a normalised form of covariance. It restricts the numbers to a certain range which shows how strong the relationship is. The most commonly used correlation coefficient is Pearson correlation coefficient\eqref{eq:2.4}. it is calculated by considering the standard deviation of both groups. This can ensure that dispersion of either attribute does not interfere our identification on the strength of mutual linear relationships\cite{ghall/pcc:2015}.

{
  \begin{equation}
    \label{eq:2.4}
    \tag{2.4}
    \rho _{X,Y}={\frac {\operatorname {cov} (X,Y)}{\sigma _{X}\sigma _{Y}}}
  \end{equation}

  \footnotesize
  where:
  \begin{itemize}[label=-, leftmargin=4em, itemsep=0.1em]
    \item ${\rho}$ is the Pearson correlation coefficient
    \item ${X, Y}$ are vectors of all the samples
    \item $\operatorname {cov}$ is the covariance
    \item ${\sigma}_{X}$ is the standard deviation of ${X}$
    \item ${\sigma}_{Y}$ is the standard deviation of ${Y}$
  \end{itemize}
}

The value of Pearson correlation coefficient is always between -1 and +1. A positive number means a positive linear correlation, and a negative number means a negative linear correlation. The closer the number towards the extremes, the stronger the relationship is. If the number is 0, it means there is no linear correlation among the pair\eqref{eq:2.5}.

\begin{equation}
  \label{eq:2.5}
  \tag{2.5}
  \text{relationship} =
  \begin{cases}
    \text{total positive linear correlation} & \text{if } \rho = 1 \\
    \text{positive linear correlation} & \text{if } \rho > 0 \\
    \text{no linear correlation} & \text{if } \rho = 0 \\
    \text{negative linear correlation} & \text{if } \rho < 0 \\
    \text{total negative linear correlation} & \text{if } \rho = -1
  \end{cases}
\end{equation}

\subsubsection{Correlation Matrix}

Given a set of data with multiple attributes, we may want to tell people how these attributes interact with each other. In addition, the result of analysis, especially in a simple regression, may not be reasonable when those features are highly dependent.

To achieve this, we can create a matrix which contains all the correlation coefficient calculated from the expanded equation\eqref{eq:2.6} with a set of given samples.

{
  \begin{equation}
    \label{eq:2.6}
    \tag{2.6}
    {\rho_{xy}={\frac {\sum _{i=1}^{n}(x_{i}-{\bar {x}})(y_{i}-{\bar {y}})}{{\sqrt {\sum _{i=1}^{n}(x_{i}-{\bar {x}})^{2}}}{\sqrt {\sum _{i=1}^{n}(y_{i}-{\bar {y}})^{2}}}}}}
  \end{equation}

  \footnotesize
  where:
  \begin{itemize}[label=-, leftmargin=4em, itemsep=0.1em]
    \item ${\rho}$ is the Pearson correlation coefficient
    \item ${n}$ is the sample size
    \item $x_{i}, y_{i}$ are the single samples indexed with i
    \item ${\bar {x}}={\frac {1}{n}}\sum _{i=1}^{n}x_{i}$ (the sample mean)
    \item ${\bar {y}}={\frac {1}{n}}\sum _{i=1}^{n}y_{i}$ (the sample mean)
  \end{itemize}
}

A correlation matrix is a symmetirc matrix to its main diagonal. The values on the main diagonal always equal to 1 because the attributes are fully dependent on themselves. Table \ref{table:cormat} gives an example of how a correlation matrix looks like.

{
  \begin{table}[ht]
    \centering
    \begin{tabular}{|c|c c c c c|}
      \hline
      Features & f1 & f2 & f3 & f4 & f5 \\ [0.5ex]
      \hline
      f1 & 1 & 0.74 & -0.38 & 0.12 & 0.43 \\
      \hline
      f2 & 0.74 & 1 & 0.26 & 0.88 & -0.57 \\
      \hline
      f3 & -0.38 & 0.26 & 1 & 0.61 & 0.59 \\
      \hline
      f4 & 0.12 & 0.88 & 0.61 & 1 & -0.22 \\
      \hline
      f5 & 0.43 & -0.57 & 0.59 & -0.22 & 1 \\
      \hline
    \end{tabular}
    \caption{Correlation Matrix}
    \label{table:cormat}
  \end{table}
}

\subsection{Principal Component Analysis}

It is always challenging to analyse a dataset with high-dimensional data points. Due to the curse of dimensionality, which was discovered by Richard Ernest Bellman in 1957, higher-dimensional space increases the difficulties of analysing and organising data exponentially\cite{keogh/cod:2017}. Especially in machine learning, given a certain number of samples, the accuracy of predictions on these samples will increase followed by the rising dimensions to a peak but then gradually drop. This is known as Hughes phenomenon\cite{hughes/itoit:1968}. Figure \ref{fig:hughes} shows how the dimensionality influence the accuracy of predictions.

\begin{figure}[ht]
  \includegraphics[width=0.8\linewidth, center]{resources/dimensionality.png}
  \caption{Hughes phenomenon}
  \label{fig:hughes}
\end{figure}

In order to reduce the dimensionality, there are two approaches can be implemented:
\begin{itemize}
  \item Feature Selection: To select a subset that is more informative or relevant among all the attributes\cite{hastie/etal:2009}.
  \item Feature Extraction: To generate new features from the initial attributes of existing data\cite{wiki/fs:2018}.
\end{itemize}

The reasons and benefits of executing dimensionality reduction can be summarised as follow:
\begin{enumerate}
  \item Computational efficiency: Fewer features mean less computation on dissimilarity between pairs of data points and lower arithmetic complexity. It also implies less storage usage as the variables in each sample decrease.
  \item Statistical generalisation: By removing noise or irrelevant information from the inputs for building models, the prediction rules can be more general among the datasets.
  \item Better explanation: Visualising a lower-dimensional space is much easier. We can effortlessly illustrate the structure of data when the dimension is lower than 3. Higher-dimensional space will be more challenging to visualise, explain and comprehend.
\end{enumerate}

Principal component analysis(PCA) is a method for feature extraction. It projects features onto a lower-dimensional space. A traditional PCA is a kind of single representation approach as opposed to classification on revealing underlying information in a lower-dimensional space with a linear function.

In practice, an optimal mapping function is usually non-linear. In order to fit the data in a non-linear way, we can apply a kernel method on top of the traditional PCA, and this is called kernel PCA. It performs a linear PCA mapping in a higher dimensional kernel Hilbert space to provide a better classification. The kernel can be a polynomial function, a radial function or other functions\cite{hessam/kpca:2014}. However, in this project, we will assume that the relationship between the dimensions (cryptocurrencies) are linear and will only use a standard linear PCA to perform the dimensionality reduction.

\subsubsection{Principal Components}

The new features derived are called principal components (PCs). They represent new orthogonal axes in an order based on the amount of information it contains.

Imagine that we have a dataset X with data points $\{{x_1} ... {x_n}\}$. Each point is a D-dimensional vector. The goal is to project the data points onto a M-dimensional space where $M < D$ and maximise the variance of data after projection. The dimensions will be the top M principal components which represent the most informative new features. M is generally determined by the following factors:

\begin{itemize}
  \item Informativity: How much information of the original features has been involved?
  \item Interpretability: Are we able to visualise or make useful attribution among the principal components?
  \item Computational efficiency: The curse of dimensionality. A large number of dimensions will decrease the speed of computation in algorithms.
\end{itemize}

To obtain the first principal component, the $\lambda_{1}$ in equation \eqref{eq:2.7} should be maximised. $u_{1}$ is then known as the first principal component. $u_{1}^{T}u_{1}$ should be equal to 1. We can then define other principal components by choosing new directions of the projection which disregards the elements that are already considered\cite{bishop/praml:2006}.

{
  \begin{equation}
    \label{eq:2.7}
    \tag{2.7}
    {u_{1}^{T}Su_{1} = \lambda_{1}}
  \end{equation}

  \footnotesize
  where:
  \begin{itemize}[label=-, leftmargin=4em, itemsep=0.1em]
    \item ${\lambda_{1}}$ is the eigenvalue
    \item ${u}$ is the eigenvector having the largest eigenvalue ${\lambda_{1}}$
    \item $S$ is the data covariance
  \end{itemize}
}

\subsection{Clustering}

Clustering, cluster analysis or data segmentation is a non-parametric algorithm in the subtree of unsupervised learning. It is used to separate data into different groups using their dissimilarities (similarities) or possible distributions. Unlike supervised learning, this type of learning algorithms does not have any indicator for assessing the quality of results, and this means that it does not have any meaning or objective itself. Instead, it discovers the distribution of data and uses the definition given by people who have the specific domain knowledge. By giving the rules for partitioning data self-defined meanings, useful information can be obtained and utilised in different domains\cite{hastie/etal:2009}.

In general, clustering can be defined into two types, parametric and non-parametric. A parametric clustering groups the clusters with a a assumed density function which is usually a Gaussian, while a non-parametric one does not have any assumed distribution, it only aims on finding natural groupings within the givien dataset.

K-means clustering and hierarchical clustering are two of the most popular methods in the non-parametric cluster analysis. In K-means clustering, we specify the number of groups we want to classify into. In contrast, hierarchical clustering does not have an initial number of clusters that we want for the result. It instead shows all the possible clusters into a tree structure and allows us to choose the number of clusters we want at the end. In this project, we will only focus on the K-means algorithm in non-parametric methods.

\subsubsection{K-means Clustering}

K-means clustering is a intuitive approach which allows us to separate data points into distinct groups. To implement this, we first need to specify an initial number of clusters - K, and then randomly assign a number (cluster) from 1 to K to each object (data point). In this case, the clusters will have two features\cite{james/itsl:2009}:
{
  \begin{enumerate}
    \item ${C_1}\cup{C_2}\cup...\cup{C_K} = \{{O_1}, {O_2},..., {O_n}\}$
    \item ${C_k}\cap{C_{k^\prime}} = \emptyset \text{,}\quad \text{for} \wedge k \neq k^\prime$
  \end{enumerate}

  \footnotesize
  where:
    \begin{itemize}[label=-, leftmargin=4em, itemsep=0.1em]
      \item ${C_k}$ is the kth cluster
      \item ${O_n}$ is the nth object
    \end{itemize}
}

These properties mean that each single object will be in exactly one cluster and the clusters does not overlap. After this initial setting, we want to optimise\eqref{eq:2.8} the grouping because the previous assignment is just a random initialisation. We want to make sure that the objects are concentrated which means the data point fit the best in the assigned cluster.

{
  \begin{equation}
    \label{eq:2.8}
    \tag{2.8}
    {{\underset {\mathbf {C_1,...,C_K} }{\operatorname {minimize} }}\left\{\sum _{k=1}^{K}\,{\frac {1}{|C_{k}|}}\,\sum _{ {i} , {i^\prime} \in C_{k}}\sum_{j=1}^{p}(x_{ij}-x_{{i^\prime}j})\right\}}
  \end{equation}

  \footnotesize
  where:
  \begin{itemize}[label=-, leftmargin=4em, itemsep=0.1em]
    \item ${C_k}$ is the kth cluster
    \item ${x_{ij}}$ is the jth attribute of the ith object
  \end{itemize}
}

To fulfil the condition above which is \eqref{eq:2.8}, we can simplify the apporach as below to classify our data points into the multiple clusters\cite{james/itsl:2009}:
{
  \begin{enumerate}
    \item Randomly assign an initial number from 1 to K to each observation.
    \item Iterate over the following steps until the assigned cluster of each observation stops changing:
    \begin{enumerate}
      \item for i in range(1, K): \\
      Compute the centroid of each cluster which is the mean of vectors with the same k (cluster) assigned.
      \item Calculate the distance (usually Euclidean distance) between each object and each of the clusters.
      \item  Assign the nearest k (cluster) to each observation.
    \end{enumerate}
  \end{enumerate}
}
Figure \ref{fig:kmeans} shows the difference after an optimisation of the cluster assignment. The colors indicate different clusters, and the groups are separated after the implementation of K-means clustering.

\begin{figure}[ht]
  \centering
  \figuretitle{K = 4}
  \begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\linewidth, center]{resources/kmeans_before.png}
    \caption{Before Iteration}
    \label{fig:before_iter}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\linewidth, center]{resources/kmeans_after.png}
    \caption{After Iteration}
    \label{fig:after_iter}
  \end{subfigure}
  \caption{K-means Clustering}
  \label{fig:kmeans}
\end{figure}

\subsubsection{Dynamic Time Warping Distance}

Euclidean distance is a metric for evaluating the distance between sequences. It is useful as it produces non-negative result and has linear time complexity. However, it is restricted by the alignment of sequences, which means that the distance can only be calculated if the two sequences are in the same length. If we want to learn how similar the shapes of two objects/lines in time series are, this is not a good method to be used.

Dynamic time warping (DTW) is a popular shape-based algorithm based on dynamic programming in time series analysis. DTW is a time distortion method which adjusts the corresponding elements in two vectors and finds the minimal distance among the neighbours. The accumulative value will then become the final distance in between. It is widely used in temporal sequence data such as speech recognition\cite{aggarwal/data:2013}.

Given two vectors $t$ and $r$, and the lengths are $M$ and $N$ respectively, DTW aims on finding a path to minimise the distance between $t$ and $r$.

\begin{figure}[ht]
    \includegraphics[width=0.8\linewidth, center]{resources/DTW_path.png}
    \caption{DTW Optimal Path}
    \label{fig:dtwpath}
\end{figure}

The calculation of the DTW distance needs to satisfy the following conditions\cite{wiki/dtw:2018}:
{
  \begin{enumerate}
    \item The first element from $t$ must match the first element from $r$.
    \item The last element from $t$ must match the last element from $r$.
    \item Given $d(i,j)$ is the distance of a point on the optimal path. The points that connect into $d(i,j)$ can only be $d(i-1,j)$, $d(i-1,j-1)$ and $d(i,j-1)$.
    \item Given any element in $t$, there must be at least one corresponding element in $r$, and vice versa.
  \end{enumerate}
}

We can use a recursion\eqref{eq:2.9} to find out the minimal accumulative distance $A(M,N)$ which is our goal of the DTW distance\cite{wiki/dtw:2018}.

{
    \begin{equation}
        \label{eq:2.9}
        \tag{2.9}
        {A(i,j) = d(i,j) + min
        \left \{
            \begin{tabular}{c}
                A(i-1,j) \\
                A(i-1,j-1) \\
                A(i,j-1)
            \end{tabular}
        \right \}}
    \end{equation}

    \footnotesize
    where:
    \begin{itemize}[label=-, leftmargin=4em, itemsep=0.1em]
        \item $D(i,j)$ is the distance between $t_i$ and $r_j$
        \item $A(i,j)$ is the accumulative distance from the starting point to $(i,j)$
    \end{itemize}
}

\section{Analysis of Cryptocurrencies}

The data of cryptocurrencies is obtained from the RESTful API \url{https://min-api.cryptocompare.com/}\cite{cryptocompare} which is free for non-profit purposes. We store the data into our local Docker based PostgreSQL database for development and convenience in case the internet or the API are unstable. Our Java programme will then update the latest data on a daily basis.

This analysis will use the historical daily OHLC (Open price, High price, Low price, Close price) data of cryptocurrencies as the initial input obtained from the API and will only consider 29 cryptocurrencies that have complete data from \date{1st January 2016} to \date{31st July 2018}. The reason why we choose the number 29 is because we want to compare the performance on cryptocurrencies with that on stocks and we will choose Dow Jones 30 as our counterpart. We want to make them have the same size of products and DWDP (DowDuPont) in Dow Jones does not have the data before \date{1st September 2017}.

\subsection{Data Preprocessing}

The original OHLC data of cryptocurrencies contains the open price, highest price, lowest price and close price of different cryptocurrencies for each day. The unit of the price is USD. There is no specific trading hours for cryptocurrencies. The metric the API used to split the values into days is based on 00:00 GMT time. For simplicity, we will use symbols to represent cryptocurrencies. Table \ref{table:symbolcrypto} shows the corresponding cryptocurrency of each symbol.

{
  \begin{table}[ht]
    \centering
    \small
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Symbol & Crypto & Symbol & Crypto & Symbol & Crypto \\
        \hline
        AEON & Aeon & BAY & BitBay & BLOCK & Blocknet \\
        BTC & Bitcoin & BTS & BitShares & BURST & Burst \\
        CRW & Crown & DASH & Dash & DGB & DigiByte \\
        DOGE & Dogecoin & EMC2 & Einsteinium & ETH & Ethereum \\
        FCT & Factom & FTC & Feathercoin & GAME & GameCredits \\
        GRS & Groestlcoin & LTC & Litecoin & MONA & MonaCoin \\
        NAV & NavCoin & NLG & Gulden & POT & PotCoin \\
        PPC & Peercoin & RDD & ReddCoin & SYS & Syscoin \\
        VIA & Viacoin & VTC & Vertcoin & XCP & Counterparty \\
        XMR & Monero & XRP & Ripple & & \\
        \hline
    \end{tabular}
    \caption{Symbols of Cryptocurrencies}
    \label{table:symbolcrypto}
  \end{table}
}

Before we start our analysis, we need to preprocess the data to ensure that the data is clean enough and in the format which can be used as the input of the analysis. The historical daily OHLC data requested from the API is on the crypto by crypto basis. In this case, we extract all the data, combine them into the format of Table \ref{table:ohlccrypto} and store them into the database. Overall, there is a huge growth among the price of most of the cryptocurrencies.

{
  \begin{table}[ht]
    \centering
    \scriptsize
    \begin{tabular}{|c|c c c c c c|}
      \hline
      Date & AEON\_open & AEON\_high & AEON\_low & ... & XRP\_low & XRP\_close \\ [0.5ex]
      \hline
      2016-01-01 & 0.01454 & 0.0217 & 0.01316 & ... & 0.005132 & 0.0055 \\
      \hline
      2016-01-02 & 0.01498 & 0.01734 & 0.01388 & ... & 0.005 & 0.005125 \\
      \hline
      2016-01-03 & 0.01378 & 0.01536 & 0.01379 & ... & 0.005 & 0.0052 \\
      \hline
      2016-01-04 & 0.01391 & 0.0143 & 0.01218 & ... & 0.0051 & 0.0051 \\
      \hline
      ... & ... & ... & ... & ... & ... & ... \\
      \hline
      2018-07-28 & 1.58 & 1.75 & 1.58 & ... & 0.4482 & 0.4576 \\
      \hline
      2018-07-29 & 1.67 & 1.78 & 1.58 & ... & 0.4503 & 0.4529 \\
      \hline
      2018-07-30 & 1.68 & 1.69 & 1.57 & ... & 0.4346 & 0.4458 \\
      \hline
      2018-07-31 & 1.55 & 1.55 & 1.42 & ... & 0.4272 & 0.4351 \\
      \hline
    \end{tabular}
    \caption{Historical OHLC of Cryptocurrencies}
    \label{table:ohlccrypto}
  \end{table}
}

Our objective is to evaluate the risk based on price volatility which is the daily price changes ($g$) or daily rate of return ($r$). We can calculate the daily price changes through dividing the close price by the open price. The rate of return ($r$) is then equal to $1 - g$. Table \ref{table:drcrypto} shows the daily price changes after transformation from the historical OHLC dataset.

{
  \begin{table}[ht]
    \centering
    \scriptsize
    \begin{tabular}{|c|c c c c c c c|}
      \hline
      Date & AEON & BAY & BLOCK & ... & XCP & XMR & XRP \\ [0.5ex]
      \hline
      2016-01-01 & 0.030949 & -0.296502 & -0.265306 & ... & -0.038405 & 0.108141 & 0.056676 \\
      \hline
      2016-01-02 & -0.073431 & 0 & 0.180451 & ... & -0.033988 & -0.153010 & 	-0.001364 \\
      \hline
      2016-01-03 & 0.002903 & -0.085601 & -0.036252 & ... & -0.025285 & 0.059783 & 0.014634 \\
      \hline
      2016-01-04 & -0.097052 & 0.124730 & 0.009491 & ... & 0.048847 & -0.035897 & -0.019231 \\
      \hline
      ... & ... & ... & ... & ... & ... & ... & ... \\
      \hline
      2018-07-28 & 0.056962 & -0.024547 & 0.012285 & ... & -0.005362 & -0.000714 & 0.006157 \\
      \hline
      2018-07-29 & 0.011976 & 0.002037 & -0.040519 & ... & -0.028340 & -0.030859 & -0.010487 \\
      \hline
      2018-07-30 & -0.023810 & -0.016599 & -0.078947 & ... & 0 & -0.025429 & -0.015677 \\
      \hline
      2018-07-31 & -0.064516 & -0.059292 & -0.094542 & ... & -0.098820 & -0.076615 & -0.024002 \\
      \hline
    \end{tabular}
    \caption{Historical Daily Returns of Cryptocurrencies}
    \label{table:drcrypto}
  \end{table}
}

In order to test how good the risk estimation is, we will use sliding window method to split the whole dataset into multiple parts. We use 100 as the size of the sliding window in this analysis. This kind of method will conduct a prediction or estimation on each part of dataset. For example, there are 500 datapoints, and the size of the window is 200. We can then estimate the $201^{\mathrm{th}}$ VaR using the first 200 datapoints and use the $2^{\mathrm{th}}$ to $201^{\mathrm{th}}$ datapoints to estimate the $202^{\mathrm{th}}$ value and so on. As a result, there will be 300 VaRs that can be predicted.

\subsection{Risk Estimation}

We will use VaR to evaluate the risk of cryptocurrencies since it provides a simple way of estimating the degree of our loss under risks. In here, we regard each cryptocurrency as a single investment instead of generating a combined portfolio because we want to construct a metric to compare the performance improvement after a clustering which will be conducted later. As a result, there will be 29 VaRs corresponding to the 29 cryptocurrencies.

\subsubsection{Historical Simulation of VaR}

A 95\% confidence level of 1-day VaR estimation will be used in this project, which means we want to know how bad our loss of the investment can become for the next day in a 95\% of confidence. A historical simulation method will be applied to the sliding windows in here to simulate the scenarios that have happened and find out the worst 5\% of the experience. The $5^{\mathrm{th}}$ percentile of the whole scenarios will then become our VaR.

{
  \begin{table}[ht]
    \centering
    \small
    \begin{tabular}{|c|c c c c c c|}
        \hline
        Crypto & 2016-04-10 & 2016-04-11 & 2016-04-12 & ... & 2018-07-30 & 2018-07-31 \\ [0.5ex]
        \hline
        AEON & 0.17212 & 0.17212 & 0.17212 & ... & 0.06261 & 0.06261 \\
        \hline
        BAY & 0.16117 & 0.15626 & 0.15626 & ... & 0.06569 & 0.06569 \\
        \hline
        BLOCK & 0.22696 & 0.19087 & 0.19087 & ... & 0.08441 & 0.08441 \\
        \hline
        ... & ... & ... & ... & ... & ... & ... \\
        \hline
        XCP & 0.13631 & 0.13631 & 0.13631 & ... & 0.07843 & 0.07843 \\
        \hline
        XMR & 0.15443 & 0.15443 & 0.15443 & ... & 0.09825 & 0.09825 \\
        \hline
        XRP & 0.08568 & 0.08568 & 0.08568 & ... & 0.07522 & 0.07522 \\
        \hline
    \end{tabular}
    \caption{VaRs of Cryptocurrencies Before Clustering}
    \label{table:varcryptobefore}
  \end{table}
}

Table \ref{table:varcryptobefore} states the 95\% 1-day VaRs of each cryptocurrency. The values are the daily returns. The numbers represent loss in percentage. Normally, the VaRs stay in the same values for several days because the historical data used for the estimation only changes slightly in every steps. When we estimate a new VaR, we only move the window one-day forward. This will not cause the $5^{\mathrm{th}}$ percentile of the returns change significantly.

\subsection{Relationship Between Investments}

There are many ways of grouping our investments, either through observation of direct correlations between daily volatilities of individual products or discovery of long-term similarity between volatility movements of the products. We will use both methods to help our construction of trading strategies because short-term and long-term hedging strategies are equally important for investors.

\subsubsection{Product Correlations}

The Pearson correlation coefficient is useful and straight forward in here for researching the direct relationships between two cryptocurrencies. It summarises the relationships of pairwise observations between two cryptocurrencies and restricts the result in the range between -1 and +1, and this allows us to observe the strength of connections on those cryptocurrencies based on the daily returns. A larger correlation coefficient means there exists a stronger correlation between their daily returns. This analysis aims on helping the construction of trading strategies. The whole dataset should be involved to generate a well-clustered hedging strategy.

We can generate a correlation matrix which states the pairwise correlations of all the cryptocurrencies. Next, a visualisation of the matrix helps us to analyse and observe the degree of correlations easier. Figure \ref{fig:cryptocorr} illustrates the result of a visualisation. The colour bar explains the meaning of different colours. Lighter colours represent a stronger correlation, and vice versa.

\begin{figure}[ht]
    \includegraphics[width=1\linewidth, center]{resources/crypto_corr.png}
    \caption{Correlation Matrix of Cryptocurrencies}
    \label{fig:cryptocorr}
\end{figure}

There is a diagonal on the chart which represents the correlations between all the cryptocurrencies and themselves. These blocks are all yellow which means the correlations are total positive linear correlation. And the chart is symmetric against the diagonal. We can look at either part to analyse the values.

BTC (Bitcoin) has some interesting features. It has extreme values against other cryptocurrencies. It has a strong correlation at around 0.7 with LTC (Litecoin) and a negative correlation at approximately -0.1 with NLG (Gulden) and MONA (MonaCoin). ETH (Ethereum) and XMR (Monero) are also strongly correlated to BTC (Bitcoin) at arround 0.6 of correlation coefficient. When it comes to XMR (Monero), it is dependent on several cryptocurrencies such as AEON (Aeon), ETH (Ethereum), FCT (Factom) and LTC (Litecoin), and this means the daily changes or returns of XMR (Monero) is similar to those cryptocurrencies.

In contrast, some of the cryptocurrencies seem to be independent and have less influence from other products. BTCD (BitcoinDark) is one of these as the colours on the bar are all close to dark blue whose correlation is around 0, and that means there is no obvious correlation in between. BAY (BitBay), BLOCK (Blocknet), DOGE (Dogecoin) and MONA (MonaCoin) have this kind of phenomenon as well.

When constructing an investment portfolio, the less dependent products should be considered because strongly correlated investments might result in great return, but it might cause enormous loss simultaneously. An unstable investment behaviour should be prevented, especially our purpose is hedging.

\subsubsection{K-means Clustering on Price Movement}

In the context of clustering, we use all the data without any split because we want to make the best use of every datapoints to illustrate the pattern of the attributes.

Before starting our clustering analysis, there are some prerequisites to be implemented. First, we need to transpose our data to make the cryptocurrencies become observations/objects and see the time-series as the features. A clustering can then group the cryptocurrencies together based on the similarity/dissimilarity of their daily returns along the timeline. After the trnaspose, a dimensionality reduction should be executed as there are too many dimensions which are dates among the transposed dataset, and this will cause a computational difficulty on K-means clustering.

We first take the daily returns dataset as our input dataset. Table \ref{table:tpcrypto} illustrates how the data looks like after a transpose. It is now a 29 by 943 table which used to be 943 by 29. The rows become the cryptocurrencies and the columns are dates from \date{2016-01-01} to \date{2018-07-31}. The columns are the attributes we will use for our clustering. We can now use the PCA (Principal Component Analysis) to reduce the number of dimensions.

{
  \begin{table}[ht]
    \centering
    \scriptsize
    \begin{tabular}{|c|c c c c c c|}
      \hline
       & 2016-01-01 & 2016-01-02 & 2016-01-03 & ... & 2018-07-30 & 2018-07-31 \\ [0.5ex]
      \hline
      AEON & 0.030949 & -0.073431 & 0.002903 & ... & -0.023810 & -0.064516 \\
      \hline
      BAY & -0.296502 & 0 & -0.085601 & ... & -0.016599 & -0.059292 \\
      \hline
      BLOCK & -0.265306 & 0.180451 & -0.078947 & ... & 0.059783 & -0.094542 \\
      \hline
      ... & ... & ... & ... & ... & ... & ... \\
      \hline
      XCP & -0.038405 & -0.033988 & -0.025285 & ... & 0 & -0.098820 \\
      \hline
      XMR & 0.108141 & -0.153010 & 0.059783 & ... & -0.025429 & -0.076615 \\
      \hline
      XRP & 0.056676 & 	-0.001364 & 0.014634 & ... & -0.015677 & -0.024002 \\
      \hline
    \end{tabular}
    \caption{Transposed Daily Returns of Cryptocurrencies}
    \label{table:tpcrypto}
  \end{table}
}

The number of Principal Components can be decided by the amount of variance that has been explained. Figure \ref{fig:cryptopca} shows the percentage of explained variance with different number of PCs. We choose the number 26 as our number of PCs because it explains around 99\% of the attributes and our objective is to make the clustering computationally efficient without losing too much information. With respect to visualisation, because we only want to know the similarity between shapes of the volatilities, we can illustrate line charts of the cryptocurrencies' PCs even on a higher dimensional space.

\begin{figure}[ht]
    \includegraphics[width=1\linewidth, center]{resources/crypto_pca_explained.png}
    \caption{Cumulative Explained Variance Percentage of PCs on Cryptocurrencies}
    \label{fig:cryptopca}
\end{figure}

After the preparation, K-means clustering can be implemented. The number of clusters, K, we select is 8 because of minimisation of the cost in each cluster and properly distribute the cryptos into clusters. The distance metric we are using is DTW (Dynamic Time Warping) because we want to know how similar the activities between cryptocurrencies are instead of the direct similarity which is restricted by strict time criterion. DTW is computationally more expensive than Euclidean distance, but it provides a more flexible way of comparing the distance between two objects.

The cryptocurrencies are distributed into 8 clusters after multiple iteration until the cost\eqref{eq:2.8} is minimised. The cluster contents are described as follow:
\begin{itemize}
    \item \textsl{Cluster 1}: DGB, MONA, POT, VIA, VTC
    \item \textsl{Cluster 2}: BTC, BTS, ETH, FCT, LTC, NLG, XMR, XRP
    \item \textsl{Cluster 3}: AEON, BLOCK, GRS
    \item \textsl{Cluster 4}: CRW
    \item \textsl{Cluster 5}: DOGE
    \item \textsl{Cluster 6}: BAY, BURST, FTC, GAME, PPC, SYS, XCP
    \item \textsl{Cluster 7}: EMC2, NAV
    \item \textsl{Cluster 8}: DASH, RDD
\end{itemize}

There is a skewness among the clusters. \textsl{Cluster 1}, \textsl{Cluster 2} and \textsl{Cluster 6} have more components than other clusters, and \textsl{Cluster 2} contains some of the most popular cryptocurrencies of the market. BTC (Bitcoin), ETH (Ethereum) and XRP (Ripple) are the top 3 cryptocurrencies in the market. \textsl{Cluster 5} only has a cryptocurrency which is DOGE (Dogecoin). This might be because people can only buy Dogecoin through the major cryptocurrencies instead of USD or EUR. Investors then regard Dogecoin as the substitute of those products when the prices of other cryptocurrencies are unstable. Apart from this, other cryptocurrencies seem to scatter around clusters.

\begin{figure}[ht]
    \includegraphics[width=1\linewidth, center]{resources/crypto_kmeans.png}
    \caption{Clusters of Cryptocurrencies}
    \label{fig:cryptokmeans}
\end{figure}

We can then look at Figure \ref{fig:cryptokmeans} which shows the result of our K-means clustering. The red lines represent the centres or the means of elements in each cluster, and the grey lines are the cryptocurrencies. Each cluster has a distinct shape of the central line. Those elements which are in the same cluster should have similar shapes of lines. The objects in \textsl{Cluster 2} are more stable than other those in other clusters. \textsl{Cluster 1}, \textsl{Cluster 3}, \textsl{Cluster 6}, \textsl{Cluster 7} and \textsl{Cluster 8} are more fluctuated. \textsl{Cluster 4} and \textsl{Cluster 5} have strange activities and have some extreme phenomena.

\subsection{Performance}

\subsubsection{Original Accuracy and p-value}

The accuracy of a 1-day VaR can be tested by comparing the VaRs with the actual losses of each day. If the actual loss is larger than the VaR, we can say that the VaR is not accurate and mark it as an incorrect estimation of VaR. By using the sliding window method, we can test how many times the daily returns exceed the 1-day VaRs and calculate the percentage of correct estimation. Table \ref{table:acccryptobefore} shows the VaR accuracy of each cryptocurrency.

{
  \begin{table}[ht]
    \centering
    \small
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Symbol & Accuracy & Symbol & Accuracy & Symbol & Accuracy \\
        \hline
        AEON & 95.02\% & BAY & 94.9\% & BLOCK & 94.31\% \\
        BTC & 93.48\% & BTCD & 94.66\% & BURST & 94.31\% \\
        CRW & 94.78\% & DASH & 94.07\% & DGB & 94.54\% \\
        DOGE & 93.59\% & EMC2 & 94.9\% & ETH & 93.12\% \\
        FCT & 94.31\% & FTC & 94.54\% & GAME & 91.93\% \\
        GRS & 94.42\% & LTC & 93.71\% & MONA & 93.59\% \\
        NAV & 94.07\% & NLG & 94.54\% & POT & 94.31\% \\
        PPC & 93\% & RDD & 94.42\% & SYS & 93.83\% \\
        VIA & 94.78\% & VTC & 93.95\% & XCP & 94.31\% \\
        XMR & 94.54\% & XRP & 94.42\% & & \\
        \hline
    \end{tabular}
    \caption{VaR Estimation Accuracies of Cryptocurrencies Before Clustering}
    \label{table:acccryptobefore}
  \end{table}
}

Most of the accuracies are lower than 95\% which is the confidence level of our VaR estimation. Only that of AEON (Aeon) with 95.02\% is slightly higher than the 95\% level of confidence. That means our VaR estimation is not accurate. It might be because the VaR estimation did not capture some of the financial events. Overall, the average of all the accuracies is around 94.15\% over the 29 cryptocurrencies and 843 days of VaR estimation.

To examine if the VaR estimation is just accidentally incorrect, we can construct a null hypothesis of how many times the VaRs are exceeded. The threshold of a 95\% 1-day VaR is around 42 days out of 843 days, which means there should be exactly 42 days of daily losses breaking the VaR estimation. We can then assume our null hypothesis as follow:

\begin{itemize}
    \item \textsl{$H_0$}: Number of VaRs being exceeded $=$ 42
    \item \textsl{$H_a$}: Number of VaRs being exceeded $\neq$ 42
\end{itemize}

The null hypothesis means that we normally believe the number of VaRs that are exceeded should be exactly 42, and the alternative hypothesis is that the number of VaRs being exceeded is unequal to 42. We can now calculate the p-value of this null hypotheis to test if the evidence of our alternative hypothesis is significant enough to reject the null hypothesis. The p-value after calculation is approximately 6.9\%. In here, we use the significance level of 5\% which also represents a confidence level of 95\%. The p-value is greater than the significance level. As a result, we retain the null hypothesis and say the evidence of that the number of VaRs being exceeded is unequal to 42 times is not significant enough to say our assumption on the null hypothesis is wrong at the 95\% level of confidence. This means the estimation of VaRs in here might be accurate enough at the significance level of 5\%.

\begin{itemize}
    \item \textsl{$H_0$}: Number of VaRs being exceeded $\geq$ 44
    \item \textsl{$H_a$}: Number of VaRs being exceeded $<$ 44
\end{itemize}

We can then test if the VaRs are underestimated using the above null hypothesis. We believe that we may underestimate the VaRs and it is not an accident that VaRs are exceeded more than 44 times. The p-value is 86.21\% which is extremely high, and this means we underestimate the VaRs and we should retain our 1-sided null hypothesis.

\begin{itemize}
    \item \textsl{$H_0$}: Number of VaRs being exceeded $\geq$ 44 or Number of VaRs being exceeded $\leq$ 40
    \item \textsl{$H_a$}: 40 $<$ Number of VaRs being exceeded $<$ 44
\end{itemize}

After the 1-sided hypothesis testing, we know that our VaRs are underestimated, and we can then use a 2-sided null hypothesis to inspect if the VaRs are also overestimated. The p-value of the null hypothesis above is approximately 86.21\% which is the same as that of 1-sided null hypothesis. This result shows that all of the VaRs are underestimated if they are not accurately estimated.

\subsubsection{Improvement of Value at Risk Estimates After Clustering}

Since we have distributed the cryptocurrencies into different clusters, we can try to use the result to improve the estimation of VaRs. Instead of calculating the VaRs of each cryptocurrency, we calculate the 95\% 1-day VaR of each cluster. For example, if there are 500 scenarios in each cryptocurrency, the VaR will be the $5^{\mathrm{th}}$ percentile of the scenarios which is the $25^{\mathrm{th}}$ worst scenario. However, if we calculate the VaR of a cluster with 5 cryptocurrencies, there will be 2500 possible scenarios. The VaR will then be the the $125^{\mathrm{th}}$ worst scenario instead. This scenario is the VaR of every cryptocurrencies in this cluster.

{
  \begin{table}[ht]
    \centering
    \small
    \begin{tabular}{|c|c c c c c c|}
        \hline
        Crypto & 2016-04-10 & 2016-04-11 & 2016-04-12 & ... & 2018-07-30 & 2018-07-31 \\ [0.5ex]
        \hline
        AEON & 0.18918 & 0.17296 & 0.17296 & ... & 0.08250 & 0.08250 \\
        \hline
        BAY & 0.12574 & 0.11807 & 0.12574 & ... & 0.08953 & 0.09228 \\
        \hline
        BLOCK & 0.18918 & 0.17296 & 0.17296 & ... & 0.08250 & 0.08250 \\
        \hline
        ... & ... & ... & ... & ... & ... & ... \\
        \hline
        XCP & 0.12574 & 0.11807 & 0.12574 & ... & 0.08953 & 0.09228 \\
        \hline
        XMR & 0.10639 & 0.11001 & 0.10913 & ... & 0.08023 & 0.08023 \\
        \hline
        XRP & 0.10639 & 0.11001 & 0.10913 & ... & 0.08023 & 0.08023 \\
        \hline
    \end{tabular}
    \caption{VaRs of Cryptocurrencies After Clustering}
    \label{table:varcryptoafter}
  \end{table}
}

Table \ref{table:varcryptoafter} illustrates the result of VaR estimation after an implementation of K-means clustering. Some of the values become larger which means the predicted loss is bigger. This is because the cryptocurrency has bigger risks in its cluster at that time under our clustering. Smaller values compared to the previous values mean that the risk of the product has been overestimated in the previous estimation at that point. We have adjusted the risks based on the result of clustering.

Similar to metric of calculating the VaR accuracies before the clustering, we can test the performance of our VaR estimation after an implementation of clustering. Table \ref{table:acccryptoafter} shows the accuracies after a K-means clustering. Red numbers indicate the cryptocurrencies that have an improvement on the VaR estimation in comparison with the accuracies before any actions. Over 50\% of the cryptocurrencies benefit from the clustering and many of them are the most popular products such as BTC (Bitcoin), ETH (Ethereum), LTC (Litecoin) and XRP (Ripple) although it results in some decrease on the other cryptocurrencies. In addition, the average accuracy is about 94.91\% which is higher than the average before clustering at 94.15\%.

{
  \begin{table}[ht]
    \centering
    \small
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Symbol & Accuracy & Symbol & Accuracy & Symbol & Accuracy \\
        \hline
        AEON & \color{red}96.09\% & BAY & 93.83\% & BLOCK & \color{red}95.14\% \\
        BTC & \color{red}97.98\% & BTCD & 93.71\% & BURST & \color{red}94.78\% \\
        CRW & 94.78\% & DASH & \color{red}98.34\% & DGB & \color{red}95.61\% \\
        DOGE & 93.59\% & EMC2 & 94.54\% & ETH & \color{red}96.44\% \\
        FCT & 93.12\% & FTC & 94.31\% & GAME & \color{red}95.02\% \\
        GRS & \color{red}94.66\% & LTC & \color{red}96.92\% & MONA & \color{red}96.56\% \\
        NAV & \color{red}95.14\% & NLG & 93.24\% & POT & \color{red}94.9\% \\
        PPC & \color{red}96.44\% & RDD & 91.22\% & SYS & \color{red}94.31\% \\
        VIA & 93.95\% & VTC & \color{red}94.19\% & XCP & \color{red}94.66\% \\
        XMR & 94.36\% & XRP & \color{red}95.49\% & & \\
        \hline
    \end{tabular}
    \caption{VaR Estimation Accuracies of Cryptocurrencies After Clustering}
    \label{table:acccryptoafter}
  \end{table}
}

We then use the same null hypothesis as the previous hypothesis testing to test if the VaR estimation is good enough to retain our null hypothesis which assumes that the number of VaRs being exceeded by the corresponding losses is equal to 42. After the calculation, the p-value is around 3.45\% which is significant enough to reject our null hypothesis at the 95\% level of confidence. That means our VaR estimation after the clustering is possibly not accurate. This result does not show the evidence of improvement after K-means clustering.

We continue to use the 1-sided hypothesis testing that we have used previously. We want to test if the VaRs are also underestimated after the K-means clustering. The p-value in here is about 55.17\% which is still very high, but it is much smaller than the previous p-value at 86.21\%. This indicates that we have decrease the percentage of underestimation. The p-value of the 2-sided null hypothesis is the same as the one before clustering at 86.21\%, and this states that the K-means clustering has transferred the underestimation to an overestimation. An overestimation is better than an underestimation because it helps us to avoid substantial loss under a financial crisis.

\section{Analysis of Stocks}

Since the K-means clustering helps our estimation of risks on cryptocurrencies, it might be also applicable to stock data. The stock data is retrieved from the RESTful API \url{https://iextrading.com/developer/}\cite{iextrading} which is also an open source. As what has been conducted on cryptocurrencies, we extract the data to our local database and use the Java programme to implement daily ETL.

The analysis of stocks will use the same criterion and standard as that of cryptocurrencies, which means the input dataset is also the historical dialy OHLC. It contains the data from \date{1st January 2016} to \date{31st July 2018} of 29 stocks in Dow Jones 30, and this excludes DWDP (DowDuPont) because there is no complete data for DWDP (DowDuPont) over the period.

\subsection{Data Preprocessing}

The data for stocks is also using USD as the unit of prices. However, there are trading hours for stocks. The stocks of Dow Jones are listed under NYSE (New York Stock Exchange) or the Nasdaq Stock Market, and the trading hours for both exchanges are 09:30 to 16:00 EST from Monday to Friday except public holidays. As a result, there is no data for weekends. Table \ref{table:symbolstock} lists the full company name of each stcok.

{
  \begin{table}[ht]
    \centering
    \small
    \begin{tabular}{|c|c|c|c|}
        \hline
        Symbol & Stock & Symbol & Stock \\
        \hline
        AAPL & Apple & AXP & American Express \\
        BA & Boeing & CAT & Caterpillar \\
        CSCO & Cisco Systems & CVX & Chevron \\
        DIS & Walt Disney & GS & Goldman Sachs \\
        HD & The Home Depot & IBM & IBM \\
        INTC & Intel & JNJ & Johnson \& Johnson \\
        JPM & JPMorgan Chase & KO & Coca-Cola \\
        MCD & McDonald's & MMM & 3M \\
        MRK & Merck \& Company & MSFT & Microsoft \\
        NKE & Nike & PFE & Pfizer \\
        PG & Procter \& Gamble & TRV & Travelers \\
        UNH & UnitedHealth Group & UTX & United Technologies \\
        V & Visa & VZ & Verizon \\
        WBA & Walgreens Boots Alliance & WMT & Walmart \\
        XOM & ExxonMobil & & \\
        \hline
    \end{tabular}
    \caption{Symbols of Stocks}
    \label{table:symbolstock}
  \end{table}
}

Similar to our previous analysis on cryptocurrencies, we also need to preprocess the data extracted from the RESTful API. An organised OHLC dataset after extraction looks like Table \ref{table:ohlcstock}. The data starts from \date{4th January 2016} because \date{1st January 2016} is a public holiday, and the following two days are Saturday and Sunday, which are day offs of the exchanges.

{
  \begin{table}[ht]
    \centering
    \scriptsize
    \begin{tabular}{|c|c c c c c c|}
      \hline
       & AAPL\_open & AAPL\_high & AAPL\_low & ... & XOM\_low & XOM\_close \\ [0.5ex]
      \hline
      2016-01-04 & 97.6663 & 100.2915 & 97.0857 & ... & 68.3743 & 69.2731 \\
      \hline
      2016-01-05 & 100.6551 & 100.7502 & 97.476 & ... & 68.7922 & 69.8633 \\
      \hline
      2016-01-06 & 95.7151 & 97.4379 & 95.0584 & ... & 68.495 & 69.282 \\
      \hline
      2016-01-07 & 93.9257 & 95.3058 & 91.7841 & ... & 67.878 & 68.1731 \\
      \hline
      ... & ... & ... & ... & ... & ... & ... \\
      \hline
      2018-07-26 & 193.9298 & 195.2751 & 192.9333 & ... & 82.5189 & 83.38 \\
      \hline
      2018-07-27 & 194.3085 & 194.5078 & 189.4356 & ... & 79.9801 & 81.0837 \\
      \hline
      2018-07-30 & 191.2293 & 191.5283 & 188.4092 & ... & 79.887 & 80.9055 \\
      \hline
      2018-07-31 & 189.6349 & 191.4685 & 188.6783 & ... & 80.6086 & 80.6779 \\
      \hline
    \end{tabular}
    \caption{Historical OHLC of Stocks}
    \label{table:ohlcstock}
  \end{table}
}

Table \ref{table:drstock} illustrates the daily retruns transfromed from the previous ohlc data - Table \ref{table:ohlcstock}. This dataset includes daily returns of each stock and is also stored in the database to help us quickly load our input dataset for analysis without transforming the data every time, which causes a computational issue.

{
  \begin{table}[ht]
    \centering
    \scriptsize
    \begin{tabular}{|c|c c c c c c c|}
      \hline
       & AAPL & AXP & BA & ... & WBA & WMT & XOM \\ [0.5ex]
      \hline
      2016-01-04 & 0.026703 & -0.007343 & -0.006224 & ... & -0.007766 & 0.015867 & -0.000515 \\
      \hline
      2016-01-05 & -0.028748 & -0.012172 & 0.000568 & ... & -0.023191 & 0.014347 & 0.012048 \\
      \hline
      2016-01-06 & 0.001393 & -0.012569 & 0 & ... & -0.011299 & 0.017126 & 0.010830 \\
      \hline
      2016-01-07 & -0.022599 & 0.008372 & -0.024711 & ... & -0.014928 & 0.032714 & 0.002236 \\
      \hline
      ... & ... & ... & ... & ... & ... & ... & ... \\
      \hline
      2018-07-26 & -0.002055 & -0.003306 & 0.011087 & ... & 0.002523 & -0.003050 & 0.004891 \\
      \hline
      2018-07-27 & -0.020565 & 0.009527 & -0.001136 & ... & 0.013436 & -0.005754 & 0.011733 \\
      \hline
      2018-07-30 & -0.010370 & -0.018300 & -0.030194 & ... & 0.011259 & 0.010000 & -0.004748 \\
      \hline
      2018-07-31 & -0.000053 & -0.015141 & 0.005418 & ... & -0.024383 & 0.002585 & -0.001591 \\
      \hline
    \end{tabular}
    \caption{Historical Daily Returns of Stocks}
    \label{table:drstock}
  \end{table}
}

\subsection{Risk Estimation}

The same methods will be used in the context of stcoks to estimate the risk of each product. The whole daily returns data is split into sliding windows with 100 days in each window and the data in each window is used to estimate the 95\% 1-day VaRs for the next days from \date{26th May 2016} to \date{31st July 2018} excluding the holidays.

\subsubsection{Historical Simulation of VaR}

The 29 by 549 estimated 95\% 1-day VaRs of the stocks are presented on Table \ref{table:varstockbefore}. In comparison with the VaRs of cryptocurrencies on Table \ref{table:varcryptobefore}, the values for stocks are proportionally much smaller than that of cryptocurrencies, and this means that the risks of investing in stocks are lower. When investors lose money on their investment of stocks, they do not suffer from the same degree of loss as their investment on cryptocurrencies.

{
  \begin{table}[ht]
    \centering
    \small
    \begin{tabular}{|c|c c c c c c|}
        \hline
        Crypto & 2016-05-26 & 2016-05-27 & 2016-05-31 & ... & 2018-07-30 & 2018-07-31 \\ [0.5ex]
        \hline
        AAPL & 0.02591 & 0.02591 & 0.02568 & ... & 0.02049 & 0.02049 \\
        \hline
        AXP & 0.01537 & 0.01537 & 0.01537 & ... & 0.01493 & 0.01514 \\
        \hline
        BA & 0.02219 & 0.02219 & 0.02219 & ... & 0.02868 & 0.02945 \\
        \hline
        ... & ... & ... & ... & ... & ... & ... \\
        \hline
        WBA & 0.02121 & 0.02121 & 0.02068 & ... & 0.02412 & 0.02412 \\
        \hline
        WMT & 0.01608 & 0.01608 & 0.01608 & ... & 0.01939 & 0.01939 \\
        \hline
        XOM & 0.01770 & 0.01770 & 0.01770 & ... & 0.01298 & 0.01298 \\
        \hline
    \end{tabular}
    \caption{VaRs of Stocks Before Clustering}
    \label{table:varstockbefore}
  \end{table}
}

The VaR estimation of stocks starts from \date{26th May 2016} which is different to the previous VaR estimation at cryptos because of the trading day restrictions of the exchanges. With respect to the variance between the VaRs, those of stocks are much smaller than those of cryptos, which means the loss can be smaller when suffering from a crash.

\subsection{Relationship Between Investments}

A popular way of grouping investments in stock products is to find the industrial relationships between them. The stocks of the companies in the same industry normally have the similar risk factor which is called industrial risk. Industrial risks can be hedged by putting investments into different industry sectors. In this part, we will also use correlations and K-means clustering to find the dissimilarities between stocks and try to see if there is an industrial causation behind the correlations.

\subsubsection{Product Correlations}

The whole stock data will be used in here to calculate the correlation matrix. There is a similar correlation matrix chart, Figure \ref{fig:stockcorr}, to the one of cryptos. They have same attributes such as yellow diagonal and symmetry against the diagonal. In comparison with the pattern of cryptos at \ref{fig:cryptocorr}, more colours are lighter in the chart of cryptos. That means there are more correlated pairs of stocks.

\begin{figure}[ht]
    \includegraphics[width=1\linewidth, center]{resources/stock_corr.png}
    \caption{Correlation Matrix of Stocks}
    \label{fig:stockcorr}
\end{figure}

GS (Goldman Sachs) and JPM (JPMorgan Chase) have a light green with a correlation coefficient of around 0.85 on their intersection which means they are highly positive correlated. This relationship is caused by their business similarity because they are both investment banking companies. They also have positive correlations with some financial companies, e.g. AXP (American Express), TRV (Travelers).

With respect to technology companies, there are connections between them including software and hardware companies. AAPL (Apple) has higher correlations with CSCO (Cisco Systems), INTC (Intel) and MSFT(Microsoft). It is also positively correlated to V (Visa), and this might be because AAPL (Apple) has Apple Pay which is relevant to credit cards and debit cards which are the products of V (Visa).

In the pharmaceutical industry, the daily returns between the stocks are highly correlated as well. For example, the colours on the blocks between PFE (Pfizer), JNJ (Johnson \& Johnson) and MRK (Merck \& Company) are lighter than others. Some of the companies producing the complementary goods like MMM (3M) and PG (Procter \& Gamble) are also correlated to this industry.

XOM (ExxonMobil) and CVX (Chevron) are both oil and gas companies. As a result, they have a large correlation. Apart from this, they have positive correlations with MMM (3M) and CAT (Caterpillar) because these companies manufacture products using oil as materials.

\subsubsection{K-means Clustering on Price Movement}

Another data preprocessing needs to be completed in this section before starting K-means clustering analysis. A transpose of the daily returns dataset is crucial because this part of analysis aims on grouping the stocks together. The stocks should be the objects or observations in here. Table \ref{table:tpstock} illustrates the data of daily returns after transposing Table \ref{table:drstock}.

{
  \begin{table}[ht]
    \centering
    \scriptsize
    \begin{tabular}{|c|c c c c c c|}
      \hline
       & 2016-01-04 & 2016-01-05 & 2016-01-06 & ... & 2018-07-30 & 2018-07-31 \\ [0.5ex]
      \hline
      AAPL & 0.026703 & -0.028748 & 0.001393 & ... & -0.010370 & -0.000053 \\
      \hline
      AXP & -0.007343 & -0.012172 & -0.012569 & ... & -0.018300 & -0.015141 \\
      \hline
      BA & -0.006224 & 0.000568 & 0 & ... & -0.030194 & 0.005418 \\
      \hline
      ... & ... & ... & ... & ... & ... & ... \\
      \hline
      WBA & -0.007766 & -0.023191 & -0.011299 & ... & 0.011259 & -0.024383 \\
      \hline
      WMT & 0.015867 & 0.014347 & 0.017126 & ... & 0.010000 & 0.002585 \\
      \hline
      XOM & -0.000515 & 0.012048 & 0.010830 & ... & -0.004748 & -0.001591 \\
      \hline
    \end{tabular}
    \caption{Transposed Daily Returns of Stocks}
    \label{table:tpstock}
  \end{table}
}

Similarly, the dimensionality needs to be reduced as 649 attributes are too many for an efficient computation. We use the PCA (Principal Component Analysis) to conduct data extraction and obtain the most informative elements from all the dimensions. The PCs (Principal Components) are ordered by their informativity. The first PC which is the most informative explains approximately 11.73\% of the variance across all the PCs.

\begin{figure}[ht]
    \includegraphics[width=1\linewidth, center]{resources/stock_pca_explained.png}
    \caption{Cumulative Explained Variance Percentage of PCs on Stocks}
    \label{fig:stockpca}
\end{figure}

The cumulative percentage of explained variance is shown on Figure \ref{fig:stockpca}. The blue bars represents the explaned variance percentage of each PC. The orange line means the cumulative percentage of the PCs in an order of the informativity, and it will approach 1 until PC29. We select 27 as the number of PCs because the percentage of explained variance is around 99\% in here.

The data has become a 29 by 27 array which means there are 27 dimensions in each stock. We can then use this dataset to implement K-means clustering. To form consistency and compare with the result of cryptocurrencies, we select the same number of K with 8. DTW (Dynamic Time Warping) is used as the distance metric, and this metric outperforms the L2 distance (Euclidean) in the context of shape comparison.

The result after implementation of K-means clustering on the transposed daily returns dataset of stocks after PCA is presented as follow:
\begin{itemize}
    \item \textsl{Cluster 1}: JNJ, KO, MCD, MRK, PFE, PG, VZ, WMT
    \item \textsl{Cluster 2}: GS, JPM
    \item \textsl{Cluster 3}: AXP, BA, INTC, NKE, WBA
    \item \textsl{Cluster 4}: AAPL, CSCO, MSFT, V
    \item \textsl{Cluster 5}: CAT
    \item \textsl{Cluster 6}: CVX, UNH, XOM
    \item \textsl{Cluster 7}: DIS, HD, IBM, TRV, UTX
    \item \textsl{Cluster 8}: MMM
\end{itemize}

There are some industrial correlations behind each cluster. In \textsl{Cluster 2}, GS (Goldman Sachs) and JPM (JPMorgan Chase) are both investment banks. Just as what we have anlalysed on the correlation matrix in Figure \ref{fig:stockcorr}, these 2 stocks are also correlated in the long-term analysis of activity similarity. \textsl{Cluster 4} also shows the similar result to the one we have analysed on the technology companies. AAPL (Apple), CSCO (Cisco Systems) and MSFT (Microsoft) are all technology companies except V (Visa), but Visa is also slightly relevant to technology as they are developing electronic devices. These results may prove that our assumption on the relation in industry is correct, which states that stocks/companies in the same industry have similar price fluctuation.

\begin{figure}[ht]
    \includegraphics[width=1\linewidth, center]{resources/stock_kmeans.png}
    \caption{Clusters of Stocks}
    \label{fig:stockkmeans}
\end{figure}

Figure \ref{fig:stockkmeans} shows the movement of the daily returns for each cluster after the implementation of K-means clustering. The returns of the stocks generally fluctuate between a small range between -0.2 and 0.2, and this is significantly smaller than the range of cryptocurrencies. \textsl{Cluster 2}, \textsl{Cluster 3}, \textsl{Cluster 5} and \textsl{Cluster 6} have a bigger range of return movement, which means investment in these clusters might cause significant gain or loss.

\subsection{Performance}

\subsubsection{Original Accuracy and p-value}

There are 549 VaRs calculated from the sliding windows with a size of 100 in each subset. These VaRs represent the estimation of the corresponding $101^{\mathrm{th}}$ day. The accuracy can then be evaluated by comparing the VaR with the actual loss at that day, and this method is the same as how we evaluate the perfomance on the analysis of cryptocurrencies. If the loss is lower than the VaR, the result will be indicated as a bad estimation.

{
  \begin{table}[ht]
    \centering
    \small
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Symbol & Accuracy & Symbol & Accuracy & Symbol & Accuracy \\
        \hline
        AAPL & 93.26\% & AXP & 94.35\% & BA & 93.62\% \\
        CAT & 93.44\% & CSCO & 93.44\% & CVX & 94.35\% \\
        DIS & 93.26\% & GS & 93.99\% & HD & 93.99\% \\
        IBM & 93.99\% & INTC & 93.44\% & JNJ & 93.08\% \\
        JPM & 94.17\% & KO & 93.44\% & MCD & 94.72\% \\
        MMM & 93.08\% & MRK & 93.26\% & MSFT & 94.17\% \\
        NKE & 95.45\% & PFE & 94.17\% & PG & 94.17\% \\
        TRV & 93.08\% & UNH & 94.35\% & UTX & 93.99\% \\
        V & 94.35\% & VZ & 93.62\% & WBA & 93.44\% \\
        WMT & 94.17\% & XOM & 93.81\% & & \\
        \hline
    \end{tabular}
    \caption{VaR Estimation Accuracies of Stocks Before Clustering}
    \label{table:accstockbefore}
  \end{table}
}

Table \ref{table:accstockbefore} illustrates the result of the analysis performance. Each stock has its corresponding estimation accuracy. Almost every stocks have a dissatisfactory estimation accuracy except NKE (Nike) with an accuracy of 95.45\%. It can be caused by the size of sliding windows which only includes 100 data points for each portion. That is, insufficiently covering the information might result in an underestimation of the risks. The average accuracy is slightly below 94\% at 93.85\%.

Again, a null hypothesis can be implemented in here to test the performance of the estimation. The $5^{\mathrm{th}}$ percentile of 549 days is at around 27 days. If the VaRs being exceeded are exactly 27 times, the VaR estimation is accurate enough. The null hypothesis and its corresponding alternative hypothesis are described as follow:

\begin{itemize}
    \item \textsl{$H_0$}: Number of VaRs being exceeded $=$ 27
    \item \textsl{$H_a$}: Number of VaRs being exceeded $\neq$ 27
\end{itemize}

In the above hypothesis testing, we generally believe that the failures of VaR estimation will be equal to 27 times. We want to prove if the evidence is strong enough to say that the number of failures can be exactly 27 times. The p-value of this null hypothesis after calculation is 0\%. The setting of confidence level is 95\%, and this means the significance level is 5\%. The p-value is smaller than the 5\% level of significance. The evidence is strong enough for us to reject the null hypothesis. It states that the VaR estimation is not accurate under the 95\% level of confidence. There is an evidence to prove that the number of VaRs being exceeded is unequal to 27.

\begin{itemize}
    \item \textsl{$H_0$}: Number of VaRs being exceeded $\geq$ 29
    \item \textsl{$H_a$}: Number of VaRs being exceeded $<$ 29
\end{itemize}

We can then use the 1-sided null hypothesis described above to test if the VaRs are underestimated. The 1-sided p-value is 96.55\% which is extremely large, and this means there is no evidence to prove that the underestimation does not exist. There is a high possibility of VaRs being underestimated.

\begin{itemize}
    \item \textsl{$H_0$}: Number of VaRs being exceeded $\geq$ 29 or Number of VaRs being exceeded $\leq$ 25
    \item \textsl{$H_a$}: 25 $<$ Number of VaRs being exceeded $<$ 29
\end{itemize}

We then inspect the 2-sided hypothesis testing to see if the VaRs in here are also overestimated. After calculation, the p-value is 100\%, and this says that we can retain the null hypothesis and the VaR estimation is probably not accurate and all of the VaRs are underestimated or overestimated. The VaR estimation in here fails to accuratly evaluate the risks.

\subsubsection{Improvement of Value at Risk Estimates After Clustering}

Although the performance of the previous VaR estimation on stocks has satisfied the hypothesis testing at the 95\% confidence level, but a p-value of 6.9\% is still not good enough. A K-means clustering before estimating the VaRs might improve the performance of the VaR estimation. Table \ref{table:varstockafter} contains the VaRs after the implementation of K-means clustering from \date{26th May 2016} to \date{31st July 2018} with a total of 549 values for each stock. Some of the stocks have same VaRs because every data points of a single cluster are used for estimating the VaRs, and the VaRs are regarded as the VaRs of every stocks under the cluster.

Some VaRs are different to the previous ones. Smaller VaRs mean the model reckons the risk used to be overestimated, and vice versa. A clustering helps us rebalance the risks by considering all the stocks with similar risks.

{
  \begin{table}[ht]
    \centering
    \small
    \begin{tabular}{|c|c c c c c c|}
        \hline
        Crypto & 2016-05-26 & 2016-05-27 & 2016-05-31 & ... & 2018-07-30 & 2018-07-31 \\ [0.5ex]
        \hline
        AAPL & 0.02346 & 0.02346 & 0.02326 & ... & 0.02061 & 0.02068 \\
        \hline
        AXP & 0.02099 & 0.02099 & 0.02076 & ... & 0.02263 & 0.02303 \\
        \hline
        BA & 0.02099 & 0.02099 & 0.02076 & ... & 0.02263 & 0.02303 \\
        \hline
        ... & ... & ... & ... & ... & ... & ... \\
        \hline
        WBA & 0.02099 & 0.02099 & 0.02076 & ... & 0.02263 & 0.02303 \\
        \hline
        WMT & 0.01541 & 0.01541 & 0.01541 & ... & 0.01523 & 0.01523 \\
        \hline
        XOM & 0.01950 & 0.01950 & 0.01950 & ... & 0.01521 & 0.01521 \\
        \hline
    \end{tabular}
    \caption{VaRs of Stocks After Clustering}
    \label{table:varstockafter}
  \end{table}
}

With regard to evaluating of the VaR estimation perfomance after the K-means clustering, Table \ref{table:accstockafter} lists the accuracies of the VaR estimation on each stock. The same metric as the previous risk estimation for performance evaluation is used in this part. We compare the VaRs with the actual losses and compute the percentage of correct VaR estimation. Red colour indicates an increase of the accuracy compared to the original performance. About 2/3 of the stocks benefit from clustering on their VaR estimation. The accuracy of KO (Coca-Cola) has a remarkable improvement with around 2.73\%. In \textsl{Cluster 4}, the VaR estimation of every stocks has improved, and all of them are related to technologies.

{
  \begin{table}[ht]
    \centering
    \small
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Symbol & Accuracy & Symbol & Accuracy & Symbol & Accuracy \\
        \hline
        AAPL & \color{red}93.62\% & AXP & \color{red}96.9\% & BA & \color{red}94.35\% \\
        CAT & 93.44\% & CSCO & \color{red}94.9\% & CVX & 93.81\% \\
        DIS & \color{red}93.99\% & GS & 93.08\% & HD & \color{red}94.17\% \\
        IBM & 93.81\% & INTC & \color{red}94.17\% & JNJ & \color{red}95.08\% \\
        JPM & \color{red}95.45\% & KO & \color{red}96.17\% & MCD & \color{red}95.26\% \\
        MMM & 93.08\% & MRK & 92.71\% & MSFT & \color{red}94.54\% \\
        NKE & \color{red}96.17\% & PFE & 93.99\% & PG & \color{red}96.72\% \\
        TRV & \color{red}94.72\% & UNH & \color{red}94.9\% & UTX & \color{red}94.54\% \\
        V & \color{red}94.72\% & VZ & 91.07\% & WBA & 91.99\% \\
        WMT & 93.99\% & XOM & \color{red}95.45\% & & \\
        \hline
    \end{tabular}
    \caption{VaR Estimation Accuracies of Stocks After Clustering}
    \label{table:accstockafter}
  \end{table}
}

The average accuracy is about 94.37\% which is higher than the estimation before clustering at around 93.85\%. The same null hypothesis is used in this part to test how convincing the VaR estimation is. The p-value of accurately estimating the VaRs in here is approximately 3.45\%, and this means we should reject the null hypothesis which states that the errors of VaR estimation are equal to 27 times. Also, the p-value is now larger than the previous p-value at 0\%.

In the 1-sided null hypothesis for evaluating if there exists an underestimation, the p-value after the K-means clustering is around 65.52\%. This number is much smaller than the previous p-value at 96.55\% although there is still no evidence to reject the null hypothesis of an underestimation.

Again, we use the 2-sided null hypothesis to test if there is an overestimation. The p-value is approximately 86.21\%, and this is larger than the 1-sided null hypothesis. That means an overestimation indeed exists. However, according to the above hypothesis testings, the performance of the VaR estimation after a clustering outperforms the one without any optimisation because the VaRs are estimated more accurately. We may now confirm that K-means clustering does improve the performance of traditional VaR estimation on stocks.

\section{Evaluation and Conclusion}
\subsection{Comparison Between Cryptocurrencis and Stocks}

Same analysis methods and processes have been used on both cryptocurrencies and stocks. The data formats are also similar except for the length of timeline and the product type. They all use OHLC data as their input. Both analysis follows the order of data preprocessing, risk estimation, clustering and risk estimation after clustering. However, the performance and results are slightly between each other.

The correlation matrix of cryptocurrencies is harder to correlate with the backgrounds of the products themselves. There is no obvious connections like the industrial relationship of stocks. Cryptocurrencies are less intuitive because of their virtual properties. Although some of the cryptocurrencies are highly positively correlated, we can not explain the causation behind them whereas the correlations behind stocks can be explained easier by their industrial relationships.

In the context of K-means clustering, both analysis suffers from skewness. Stocks spread into multiple clusters more evenly whereas cryptocurrencies seem to be more centralised. The reason of these phenomena might be an inappropriate choice of K. The number of cluster was not optimally chosen for each dataset because we tried to use the same metrics and compare the result of analysis on both datasets.

{
  \begin{table}[ht]
    \centering
    \small
    \begin{tabular}{|c|c|c|P{1.2cm}|P{1.2cm}|}
        \hline
         & \multicolumn{4}{c|}{Before Clustering} \\ [0.5ex]
        \hline
        Product Type & Average Accuracy & p-value & 1-sided p-value & 2-sided p-value \\ [0.5ex]
        \hline
        Cryptocurrencies & 94.15\% & 0.06897 & 0.86207 & 0.86207 \\
        \hline
        Stocks & 93.85\% & 0 & 0.96552 & 1 \\
        \hhline{=====}
         & \multicolumn{4}{c|}{After Clustering} \\ [0.5ex]
        \hline
        Product Type & Average Accuracy & p-value & 1-sided p-value & 2-sided p-value \\ [0.5ex]
        \hline
        Cryptocurrencies & 94.91\% & 0.03448 & 0.55172 & 0.86207 \\
        \hline
        Stocks & 94.37\% & 0.03448 & 0.65517 & 0.86207 \\
        \hline
    \end{tabular}
    \caption{Performance Comparison Between Cryptocurrencies and Stocks}
    \label{table:comparison}
  \end{table}
}

Table \ref{table:comparison} compares the results of the risk estimation between cryptocurrencies and stocks. The risk estimation before clustering performs better on the data of cryptocurrencies in the aspect of accuracy and the p-value. The 93.85\% accuracy in stocks seem to be less convincing because of a lower p-value.

After a K-means clustering, both risk estimations improve significantly. The result of analysis on cryptocurrencies outperform that of stocks. The improvement accuracy on the cryptocurrencies is approximately 0.76\%, but the p-value dropped from 0.06897 to 0.03448 which is a 0.03449 decrease. The optimisation on the analysis of stocks performs better in the context of accuracy, as well as the p-value. K-means clustering seems to be more effective on the stocks because the improvement is more convincing when we look at the p-values although the increase of the accuracy on cryptocurrencies is higher then that of stocks which is around 0.52\%.

\subsection{Self Assessment}

Despite the analysis has produced a satisfactory result, there are some drawbacks and insufficient consideration among the whole process. The actions need to be completed can be organised as follow:

\begin{itemize}
    \item \textbf{Systematic process for crypto selection}: The 29 cryptocurrencies we used were selected by inspecting if the data is complete in the specific period. As most of the cryptocurrencies emerged in the past 2 years, the cryptocurrencies that can be analysed beginning from \date{1st January 2016} is limited. We need to select the products for analysis based on their backgrounds instead of filtering out the cryptocurrencies by the completeness. Some of the most popular cryptocurrencies might be launched later, but they are important for comprehension of the market behaviour.
    \item \textbf{Better way of reducing the dimensionality}: Currently, we are using PCA for dimensionality reduction, but the features might lose the interpretability. The original dimensions represent the dates and are in orders. When we extract the information for the features and transform them to the PCs, they become unordered and not interpretable.
    \item \textbf{Enforcement of exactly same metrics on both analysis}: The size of dates and time is different between the data of cryptocurrencies and stocks because of different trading hours. The market of cryptocurrencies operate 24 hours a day without any gaps whereas that of stocks only operates in certain hours of a day and stops working on holidays. This might result in a price incontinuity on the data of stocks.
    \item \textbf{Consideration of other methods for VaR estimation}: This project only considers historical simulation as the method of estimating VaRs. There are still other approaches for VaR estimation such as Monte Carlo methods.
    \item \textbf{Consideration of other clustering algorithms}: We should try other algorithms for time-series clustering and compare the performance with that of K-means clustering such as hierarchical clustering.
    \item \textbf{Modularised code for analysis}: The analysis is now implemented through the Jupyter notebook and Python. This is an ad hoc analysis process because the code is written in a scripting way without generating reusable methods. We should create a modularised codebase for scalable and reusable units.
\end{itemize}

\subsection{Further Work}

This project can be develped further in the future. Some of the tasks have not been completed because of the time and project size limitation. A single page web application can be built though construction of a RESTful API on the Java programme and a React.js based front-end, and this can be simply implemented by the Spring Boot. This website will visualise the result of the analysis. Also, we should separate the services into components. For example, we can split the analysis service and ETL service.

The whole analysis can be transformed into a near real time analysis on a daily basis. We can finish the analysis part of the Java programme and organise the routine tasks. The daily analysis can then be applied to trading strategy generations. In addition, another service for connecting to the exchanges and trading can be built. This service can use the trading strategies to implement a real time algo trading. Finally, a code refactoring should be executed to improve the maintainability of the programme.

\section{Professional Issues}

As an analyst, researcher and investor, we have the responsibilities to ensure our result brings positive impact on the society and improve the environment at the same time. This section will mainly discuss about the issues that we might encounter in the financial area.

\subsection{Ethical and Socially Responsible Investing}

Socially Responsible Investing (SRI) is investment behaviour that considers both financial returns and social/environmental freindlies. A basic way of choosing the investments is to use an exclusionary or inclusionary SRI filter. The most popular approach is an exclusionary method. It filters out the products or companies that are involved in violations of labor norms or violence that has impact on the peace of societies. Products that are reckoned to harm people's health are also considered to be excluded. The products normally include weapons, alcohol, tobacco and gambling. The inclusionary method is harder to be developed because people need to adjust the weights of investments in a portfolio based on how socially responsible the company is\cite{Berry2013}.

\subsection{Cryptocurrencies and Business Ethics}

Cryptocurrencies are considered as threats of traditional centralised bank system because of its decentralised attributes. They cannot be regularised by the government, and the liquidity and prices cannot be manipulated easily by a small group of people. There are difficulties on the legal regulation and assessment on cryptocurrencies. They harm the conventional payment systems and existing financial and monetary policies.

The societal and business risks can be classified into 3 levels, micro, meso and macro. Micro level focuses on individuals. The major risks come from attacks of hackers who try to steal or take cryptocurrencies illegally through an unauthorised way. Risks on the meso level emphasise that cryptocurrencies cannot replace the functionality of standard currencies. The volatilities are the main consideration of whether a company will accept the cryptocurrencies. Companies, institutions and governments are on the macro level. Because of lacking regulation, organisations tend to use cryptocurrencies for speculation. They try to escape the additional payment to the governments, and this might lead to economic collapse\cite{Dierksmeier2016}.

\subsection{Ethics and Data Analysis}

One of the research ethics raised by Robinson and Moulton (1985) states the falsification and misinterpretation of the results, and this is relevant to the ethics of data analysis. Gibbons (1973) also suggests that data collection, data presentation and data interpretation are the main elements of ethical consideration in data analysis. Researchers and analysts should define the success of a research as rejecting the proposed null hypothesis and the desired results. They also need to confront the failure of the research and interpret it properly\cite{10.2307/1177102}.

\section{Appendix}
\subsection{Programme Usage}

This section explains how to use the programme under a development purpose. Users should follow this manual in orders. The instruction only illustrates the steps of the programme for the current version in linux/unix commands.

\subsubsection{Prerequisite}

Before starting the services, the following softwares/tools should be installed.

\begin{itemize}
    \item Python - \url{https://www.python.org/downloads/}
    \item Jupyter Notebook - \url{http://jupyter.org/install}
    \item Java - \url{https://docs.oracle.com/javase/8/docs/technotes/guides/install/install_overview.html}
    \item Maven - \url{https://maven.apache.org/install.html}
    \item Docker - \url{https://docs.docker.com/install/}
    \item Docker Compose - \url{https://docs.docker.com/compose/install/}
    \item LaTeX - \url{https://www.latex-project.org/get/}
\end{itemize}

\subsubsection{Database}

Create a file called \textsl{\textbf{.env-dev}} in the \textsl{\textbf{$<$root$>$}} directory and insert the following configuration:

\begin{lstlisting}[language=bash]
    POSTGRES_DB=postgres
    POSTGRES_USER=postgres
    POSTGRES_PASSWORD=Crypto01
\end{lstlisting}

\noindent
Execute the following commands:

\begin{lstlisting}[language=bash]
    #!/bin/bash
    cd <root>/
    docker-compose up
\end{lstlisting}

\subsubsection{Server}

Execute the following commands:

\begin{lstlisting}[language=bash]
    #!/bin/bash
    cd <root>/parent/
    mvn clean install
    cd <root>/analysis/
    mvn exec:Java
\end{lstlisting}

\subsubsection{Experiment}

Execute the following commands, and then open the \textsl{\textbf{var\_crypto.ipynb}} or \textsl{\textbf{var\_stock.ipynb}} file in the browser.:

\begin{lstlisting}[language=bash]
    #!/bin/bash
    cd <root>/experiment/
    jupyter notebook
\end{lstlisting}

\subsubsection{Report}

Execute the following commands to compile the LaTeX file:

\begin{lstlisting}[language=bash]
    #!/bin/bash
    cd <root>/report/
    pdflatex Report.tex
    cp Report.pdf ../Report.pdf
\end{lstlisting}

\bibliographystyle{plain}
\bibliography{bibliography}
\end{document}
